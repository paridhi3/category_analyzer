{
  "Case Study 1.pptx": {
    "file_name": "Case Study 1.pptx",
    "project_title": "Untitled Project",
    "domain": "Retail Pharmacy",
    "client_name": "Unknown",
    "technology_used": "Collibra, Core JAVA API, ServiceNow, Ataccama",
    "summary": "**Comprehensive Summary of the Case Study**\n\n**Client Challenges**\n\nThe client, a global leader in retail pharmacy, faced significant challenges due to the absence of a unified, referenceable source for corporate data definitions. Key issues included:\n\n- **Lack of a Unified Business Glossary:** There was no centralized glossary for business and financial terms or report fields, leading to inconsistencies and confusion.\n- **Limited Report Traceability:** The organization struggled to identify which reports utilized specific data definitions, making it difficult to assess report value and trustworthiness.\n- **Unclear Calculations and KPIs:** There was no accurate, transparent view of the calculations or algorithms underpinning company metrics and KPIs.\n- **Poor Data Lineage:** The ability to trace data back to its original source was lacking, impeding data quality and compliance efforts.\n- **Ineffective Data Governance:** The company needed to optimize, protect, and leverage data as a corporate asset throughout its lifecycle, but lacked intelligent data governance mechanisms.\n\n**Solution Implemented**\n\nTo address these challenges, a comprehensive data governance and management solution was implemented, focusing on the following areas:\n\n- **Data Catalog and Dictionary Creation:** Developed detailed data catalog content and data dictionaries for both raw and analytics datasets, alongside a business glossary and KPI definition documentation.\n- **Business and Functional Requirements:** Supported the implementation process by liaising with technical teams and driving the definition of business and functional requirements, particularly for Collibra artefact readiness.\n- **Process and Workflow Design:** Designed processes and workflows, including governance and stewardship frameworks, data classification schemes, and a Data Helpdesk for user support.\n- **Metadata Lineage and Visibility:** Established end-to-end lineage and enhanced visibility of metadata, enabling users to trace data from origin to consumption.\n- **Data Sharing Policies:** Applied policies governing data sharing terms for relevant datasets to ensure compliance and proper data usage.\n\n**Client-Specific Deliverables**\n\n- **Content Creation:** Developed content for over 40 datasets, mapping both their physical and business layers, and establishing comprehensive lineage.\n- **KPI and Measure Redefinition:** Redefined more than 80 KPIs and 130 measures, documenting their lineage and calculation logic.\n- **Custom Workflows:** Built custom workflows (leveraging Core JAVA APIs) for proposing business terms and managing account processes, such as charts of accounts.\n- **Business Glossary Expansion:** Created and maintained a glossary of over 400 business terms to standardize language and definitions across the organization.\n- **Ongoing Integrations:** Initiated integration efforts with ServiceNow, Ataccama DQ Profiles, and the implementation of data sharing policies.\n\n**Impact Delivered**\n\nThe solution enabled the client to become a trusted, global innovator in the retail pharmacy sector by:\n\n- Establishing a single source of truth for data definitions and business terms.\n- Enhancing data governance, traceability, and compliance.\n- Improving the reliability and transparency of business reports and KPIs.\n- Facilitating better data-driven decision-making and operational efficiency across the organization.",
    "category": "Data Governance"
  },
  "Case Study 8.pptx": {
    "file_name": "Case Study 8.pptx",
    "project_title": "Untitled Project",
    "domain": "Automotive",
    "client_name": "Unknown",
    "technology_used": "Alation, SQL Server, Oracle, Snowflake, Amazon S3, Athena, Postgres, Looker, Tableau, Power BI",
    "summary": "**Comprehensive Summary of the Case Study**\n\n**Client Challenges**\n\nThe client, a US multinational provider of data and technology in Dealer Services, faced several significant data governance challenges:\n\n- **Lack of a Unified Data Source:** There was no centralized, referenceable repository for corporate data definitions, including a unified glossary of business terms and report fields.\n- **Limited Report Traceability:** The client was unable to easily determine which reports utilized specific data definitions.\n- **Compliance Risks:** There was no systematic identification or recording of Personally Identifiable Information (PII) data elements, which is essential for meeting legal and compliance requirements.\n- **Insufficient Cross-Domain Knowledge:** The organization lacked visibility into cross-domain data and reference data, impeding holistic data management.\n- **Need for Intelligent Data Governance:** The client required a solution to optimize, protect, and leverage data as a corporate asset throughout its lifecycle.\n\n**Genpact\u2019s Solution**\n\nGenpact addressed these challenges by implementing a comprehensive data governance solution, leveraging Alation Data Governance tools and APIs. The key steps included:\n\n- **Data Catalog and Business Glossary Creation:** Developed a data catalog and data dictionary for various datasets, and established a business glossary using Alation, making use of Alation APIs for integration and automation.\n- **PII Identification and Classification:** Defined and implemented custom rules to identify and classify PII data elements across both Product and Technology domains.\n- **Requirements Definition:** Outlined business and functional requirements for Alation artifacts, focusing on governance and stewardship design, as well as data classification.\n- **API and Data Table Mapping:** Captured and documented the relationship between product APIs and the underlying data tables they utilize.\n- **Metadata Lineage and Visibility:** Established end-to-end data lineage, providing comprehensive visibility into metadata flows.\n- **Implementation Support:** Provided ongoing support and coordination with technical teams to ensure successful implementation.\n\n**Solution Implementation**\n\nThe client ingested technical and reporting metadata from a wide range of data sources and reporting tools, including:\n\n- SQL Server\n- Oracle\n- Snowflake\n- Amazon S3 Athena\n- Postgres\n- Looker\n- Tableau\n- PowerBI\n\nThis process resulted in the creation of content for over 16 datasets, along with their respective data lineage.\n\n**Key Outcomes and Impact**\n\n- **PII Data Elements Identified:** Over 7,000 PII data elements were identified, significantly aiding the Legal and Compliance teams in meeting regulatory requirements.\n- **Business Glossary Established:** More than 200 business glossary terms were defined, promoting a common understanding of business terminology across the organization.\n- **KPI and Measure Lineage:** Work is ongoing to define key performance indicators (KPIs) and measures, and to establish their lineage.\n- **Enhanced Data Governance:** The solution provided the client with improved data governance capabilities, enabling them to optimize, protect, and leverage their data assets more effectively.\n\n**Conclusion**\n\nThrough Genpact\u2019s intervention, the client overcame critical data governance challenges by establishing a unified data catalog, improving PII management, enhancing metadata visibility, and laying the foundation for ongoing data stewardship and compliance. This has positioned the client to better manage their data as a strategic corporate asset.",
    "category": "Data Governance"
  },
  "Data Governance  solution for a US Healthcare client (dental equipment Case Study 7.pptx": {
    "file_name": "Data Governance  solution for a US Healthcare client (dental equipment Case Study 7.pptx",
    "project_title": "Untitled Project",
    "domain": "Healthcare",
    "client_name": "Unknown",
    "technology_used": "Azure Data Lake, SQL, Azure SQL, Microsoft Purview",
    "summary": "**Summary of Case Study: Data Governance Solution for a US Healthcare Client (Dental Equipment Manufacturer)**\n\n**Business Challenge**\n\nThe client, a US-based dental equipment manufacturer in the healthcare sector, faced significant data governance challenges due to the absence of a unified, referenceable source for corporate data definitions. Key issues included:\n\n- Lack of a unified business glossary for business and financial terms.\n- Inability to identify which data assets (such as tables and columns) used specific data definitions.\n- Uncertainty regarding the value and trustworthiness of each data table.\n- Difficulty in tracing data back to its original source through data lineage.\n\n**Solution Provided**\n\nGenpact delivered comprehensive, end-to-end support to address these challenges by implementing a robust data governance solution using Microsoft Purview. The main solution highlights included:\n\n- Identification and integration of data sources across Azure Data Lake, SQL, and Azure SQL databases.\n- Configuration and scheduling of scanners to capture and refresh metadata from these sources.\n- Onboarding of various data assets\u2014including business glossaries, systems, datasets, and attributes\u2014into Microsoft Purview.\n- Enrichment of technical assets with business metadata within the data catalog to enhance context and usability.\n- Capturing insights related to catalog adoption, inventory, ownership, and curation to monitor and improve data governance practices.\n- Establishment of classification rules for data assets to ensure proper categorization and compliance.\n- Creation of end-to-end data lineage, providing full visibility into the flow and transformation of data across systems.\n\n**Impact Delivered**\n\nThe implementation of this data governance solution resulted in several tangible benefits for the client:\n\n- Creation of content and lineage mapping for over 40 datasets, covering both physical and business layers.\n- Refinement of asset definitions and establishment of clear data lineage for improved traceability.\n- Development of a comprehensive business glossary with over 300 terms, standardizing business and financial terminology.\n- Identification of Personally Identifiable Information (PII) across various datasets, enhancing data privacy and compliance.\n\n**Conclusion**\n\nThrough Genpact\u2019s data governance initiative, the client achieved a unified, trusted, and referenceable data environment. This enabled better data understanding, improved compliance, and enhanced decision-making capabilities across the organization.",
    "category": "Data Governance"
  },
  "Data Governance Solution Case Study 5.pptx": {
    "file_name": "Data Governance Solution Case Study 5.pptx",
    "project_title": "Untitled Project",
    "domain": "Data Governance",
    "client_name": "Unknown",
    "technology_used": "Snowflake, EDC, AXON, IDQ, Oracle, Aurora, AWS S3",
    "summary": "**Summary of the Data Governance Case Study**\n\n**Challenges:**\nThe organization faced several challenges in implementing a comprehensive Data Governance (DG) solution. These included:\n- Documenting business assets and configuring technical assets within the data governance framework.\n- Establishing technical data lineage across a complex, multi-layered data flow, tracing data from its source through to the publish layer.\n- Creating a DG solution that not only enriches organizational data but also enables business users and data stewards to trace data back to its source and assess data quality through defined rules and quality scores.\n\n**Solution:**\nTo address these challenges, a multi-faceted approach was adopted:\n- **Source System Identification and Metadata Scanning:** Various source systems were identified, and metadata scanners were configured and scheduled to detect and capture changes in metadata from these sources.\n- **Advanced and Custom Scanners:** Advanced scanners, such as those for Snowflake, were set up. Additionally, a custom lineage scanner was developed to automate the setup of data lineage within the Enterprise Data Catalog (EDC).\n- **Business Metadata Enrichment:** Technical assets in the data catalog were enriched with business metadata, enhancing their value and usability.\n- **Business Glossary Integration:** A Business Glossary Scanner for AXON was implemented to ensure that newly added glossary items were updated in EDC.\n- **Onboarding of Data Governance Facets:** Key facets such as Business Glossary, Systems, Datasets, Attributes, and Data Quality (DQ) rules were onboarded into AXON.\n- **Relationship Mapping:** Relationships were established between physical data elements in EDC and business terms in AXON, ensuring alignment between technical and business perspectives.\n- **Tool Integration:** Integration of Informatica Data Quality (IDQ), EDC, and AXON was achieved to provide a holistic DG solution.\n- **Data Quality Score Capture:** Scores for identified DQ rules from IDQ were captured and reflected in AXON, providing visibility into data quality.\n\n**Impact Delivered:**\nThe implementation of this solution delivered significant results:\n- **Comprehensive Documentation:** Over 50 datasets, 500+ DQ rules, and 300+ business glossary terms were documented.\n- **Automated Metadata Updates:** Metadata was kept up-to-date through scheduled scanners for various data sources, including Oracle, Aurora, flat files, AWS S3, and Snowflake.\n- **Custom Data Lineage:** A custom data lineage scanner was developed, enabling the recognition and upload of custom data lineage from template files into the data catalog.\n- **Business Metadata Enrichment:** More than 50 data domain rules were established, further enriching business metadata.\n- **Continuous Data Quality Monitoring:** Scores for over 500 DQ rules were updated and refreshed daily, ensuring ongoing data quality monitoring and improvement.\n\n**Conclusion:**\nThrough a combination of advanced tooling, automation, and integration, the organization successfully established a robust Data Governance solution. This enabled comprehensive documentation, enhanced data traceability, improved data quality monitoring, and enriched business metadata, ultimately empowering business users and data stewards to make more informed, data-driven decisions.",
    "category": "Data Governance"
  },
  "Data Quality led Data Governance organization operationalizationCase Study 6.pptx": {
    "file_name": "Data Quality led Data Governance organization operationalizationCase Study 6.pptx",
    "project_title": "Untitled Project",
    "domain": "Data Governance",
    "client_name": "Unknown",
    "technology_used": "Collibra, Informatica, IBM",
    "summary": "**Summary of the Case Study: Data Quality-Led Data Governance Transformation**\n\n**Business Challenges:**\nThe organization faced several critical data management issues:\n- There was a pervasive lack of trust in enterprise data, undermining business confidence and decision-making.\n- Data ownership was unclear, leading to accountability gaps and inefficiencies.\n- The enterprise data services organization was not being utilized effectively across the company.\n- There was no comprehensive view of how data changes impacted upstream and downstream systems, increasing operational risk.\n- The business required significant improvements in data quality across multiple domains.\n- There was a need to operationalize a data governance organization with a focus on data quality.\n\n**Solution Highlights:**\nTo address these challenges, the organization implemented a comprehensive data governance solution:\n- Established and operationalized a Data Governance organization and operating model, leveraging leading data governance platforms such as Collibra, Informatica, and IBM.\n- Developed a Data Governance Handbook to document policies, procedures, and best practices, ensuring consistent governance across the enterprise.\n- Deployed a data quality framework to proactively monitor and track data quality issues.\n- Enhanced enterprise-wide trust in data by implementing clear and visible data lineage for data elements across various databases, supplemented by a business glossary.\n- Created visualizations (referred to as the \"Data Cockpit\") to communicate key data governance metrics and trends over time, demonstrating the effectiveness of the governance initiatives.\n\n**Impact Delivered:**\nThe transformation delivered significant, measurable results:\n- Cataloged approximately 1,800 Critical Data Elements (CDEs) and implemented around 6,000 data quality rules across 12 business domains within just 8 months.\n- Achieved clear visibility into the data journey throughout the enterprise, integrating data quality metrics directly into business processes.\n- Established agile data governance processes, resulting in transparent data ownership and improved collaboration across data domains.\n\n**Conclusion:**\nBy operationalizing a data quality-led data governance organization, the company addressed foundational data management challenges, improved data quality and trust, and established sustainable, transparent, and agile governance processes across the enterprise.",
    "category": "Data Governance"
  },
  "Data Quality program for a large US-based Aviation Case Study 4.pptx": {
    "file_name": "Data Quality program for a large US-based Aviation Case Study 4.pptx",
    "project_title": "Data Quality program for a large US-based Aviation organization",
    "domain": "Aviation",
    "client_name": "Unknown",
    "technology_used": "Informatica Analyst, Informatica DQ, Informatica PowerCenter",
    "summary": "**Summary of the Data Quality Program for a Large US-based Aviation Organization**\n\n**Business Challenge:**\nThe organization, with a global footprint, faced significant data management challenges due to data being dispersed across multiple locations and lacking adherence to consistent standards and rules. This fragmentation posed a risk of failing to identify top business customers and made it difficult to detect non-standard names and addresses. The organization also needed to validate address data using Address Doctor licenses and improve overall data quality.\n\n**Solution Approach:**\nTo address these challenges, a comprehensive data quality program was implemented with the following key components:\n\n- **Rule Development:** Custom rules were developed based on detailed data profiling to enable effective data cleansing, standardization, and validation.\n- **Data Set Cleansing:** Common data sets were identified and cleaned through join and overlap analysis, ensuring consistency and removing duplicates.\n- **Fuzzy Matching:** Non-conventional data quality techniques, such as fuzzy matching, were used to build standard groups and resolve inconsistencies in data entries.\n- **Reference Data Utilization:** Informatica-provided reference tables were leveraged for standardizing international data, especially where the client lacked specific licenses.\n- **Data Profiling:** The data sources were thoroughly profiled to assess and improve data uniqueness, accuracy, integrity, reasonableness, precision, and timeliness.\n- **Intermediary Data Store (IDS):** An IDS was created to store cleansed data after processing, which then served as a reliable input for the client\u2019s Master Data Management (MDM) system.\n- **Technology Stack:** The solution utilized Informatica Analyst, Informatica Data Quality (DQ), and Informatica PowerCenter (ETL) tools.\n\n**Impact and Benefits:**\nThe data quality integration enabled the organization to proactively monitor and cleanse data across all business units, leading to several benefits:\n\n- Simplified identification, measurement, and cleansing of data quality issues.\n- Enhanced ability to identify top business customers and standardize critical data elements.\n- Improved stability and reliability of the organization\u2019s products through robust data cleansing and standardization processes.\n\nOverall, the program established a strong foundation for enterprise-wide data quality management, supporting better business decisions and operational efficiency.",
    "category": "Data Quality"
  },
  "Data profiling, cleansing & future state design for a Large Financial Institution Case Study 2.pptx": {
    "file_name": "Data profiling, cleansing & future state design for a Large Financial Institution Case Study 2.pptx",
    "project_title": "Data profiling, cleansing & future state design for a Large Financial Institution",
    "domain": "Procurement",
    "client_name": "Unknown",
    "technology_used": "Informatica Analyst, Informatica DQ",
    "summary": "**Summary: Data Profiling, Cleansing & Future State Design for a Large Financial Institution**\n\n**Challenge / Business Case**\n\nA large financial institution faced significant data management challenges due to the absence of formal Data Governance and Data Quality organizations, with no clearly defined roles, responsibilities, or supporting tools. Data management processes were inconsistently applied\u2014some business units (BUs) managed data locally, while others used a centralized approach. The organization lacked a global data dictionary and master data standards, resulting in inaccurate and incomplete vendor master data, which negatively impacted spend visibility. Risk management was siloed, limiting comprehensive risk oversight. Additionally, 60% of new vendors were onboarded by Finance without Procurement\u2019s involvement, often after the fact, further exacerbating data quality and governance issues.\n\n**Solution Highlights**\n\nTo address these challenges, the institution undertook a comprehensive data profiling and one-time cleansing initiative to ensure current data was fit for purpose. A future state was designed encompassing processes, metrics, controls, technology, data quality, and governance to maintain ongoing data integrity. Specific future state processes were developed for vendor creation, maintenance, data quality, and governance. The establishment of a Business Data Services (BDS) organization was recommended to centrally manage all master data, including the vendor master (VM). A global data dictionary was created, vendor records were profiled and cleansed, and Informatica Analyst and Informatica Data Quality (DQ) tools were leveraged to support these efforts.\n\n**Impact Delivered**\n\nThe initiative delivered substantial business benefits:\n- **Spend visibility improved by 30%,** resulting in a business impact of approximately $9 million.\n- **Productivity gains** were realized through the deployment of a self-serve vendor master tool, yielding an estimated $5 million in business impact.\n- **Parent-child linkage in the vendor master** was projected to increase from 7% to 23%, enhancing data relationships and reporting.\n- **Inactive vendors were expected to decrease** from 46% to less than 5%, streamlining the vendor database and improving data quality.\n\nOverall, the project established a robust data governance and quality framework, standardized master data management, and delivered significant operational and financial improvements.",
    "category": "Master Data Management"
  },
  "Improved data quality of the \u2018client\u2019 data by cleansing, enriching Case Study 3.pptx": {
    "file_name": "Improved data quality of the \u2018client\u2019 data by cleansing, enriching Case Study 3.pptx",
    "project_title": "Untitled Project",
    "domain": "Insurance",
    "client_name": "Unknown",
    "technology_used": "D&B, Informatica Analyst, Informatica DQ",
    "summary": "**Summary of Case Study: Improved Data Quality of \u2018Client\u2019 Data for a Diversified Global Insurer**\n\n**Challenge / Business Case:**  \nA diversified global insurer faced significant data quality issues within its client data, necessitating comprehensive data profiling and cleansing to ensure adequacy and reliability. The assessment revealed that approximately 70% of the data could be cleansed automatically using a tool-based approach (specifically, Dun & Bradstreet - D&B), while the remaining 30% required manual intervention. Additionally, it was essential to define the critical data elements and identify appropriate sources for data cleansing and enrichment.\n\n**Solution Highlights:**  \n- **Data Extraction and Profiling:** Data was extracted from all regions, and a finalized dataset was prepared for cleansing and enrichment. The data sources were profiled to assess key quality dimensions, including uniqueness, accuracy, integrity, reasonableness, precision, and timeliness. A baseline data dashboard was created to monitor these metrics.\n- **Automated Cleansing and Enrichment:** The majority of the data (70%) was cleansed and enriched using D&B tools. This process included matching and deduplication of records to eliminate redundancies.\n- **Manual Cleansing:** The remaining 30% of the data, which could not be addressed automatically, was cleansed and enriched manually using internal sources and web-based research.\n- **Finalization and Sign-off:** The fully cleansed and enriched data file was further validated using external parties, and final approval was obtained from the business stakeholders.\n- **Tools Used:** Informatica Analyst and Informatica Data Quality (DQ) tools were leveraged throughout the process.\n\n**Impact Delivered:**  \n- **Data Completeness:** The initiative reduced data completeness issues by 70%, significantly enhancing the user experience.\n- **Downstream Issues:** There was a 55% reduction in downstream data-related issues, leading to smoother business operations.\n- **Compliance:** The improved data quality contributed to better regulatory compliance.\n\n**Conclusion:**  \nThrough a combination of automated and manual data cleansing and enrichment, the insurer achieved substantial improvements in client data quality, resulting in enhanced operational efficiency, user satisfaction, and compliance.",
    "category": "Data Quality"
  },
  "Legg_Mason_Case_Study 2.pptx": {
    "file_name": "Legg_Mason_Case_Study 2.pptx",
    "project_title": "AWS enterprise data hub for leading investment & assets management firm",
    "domain": "Investment Management",
    "client_name": "Unknown",
    "technology_used": "AWS, AWS S3, AWS Redshift, Apache Spark, EMR, Lambda, Step Function, Data Pipeline, Tableau, GIT, Jenkins, Salesforce",
    "summary": "**Summary of AWS Enterprise Data Hub Implementation for Leading Investment & Asset Management Firm**\n\n**Challenge & Objective:**  \nThe investment and asset management firm sought to modernize its data architecture to support current and future business needs. The primary challenge was to create a robust enterprise data hub capable of integrating diverse data sources\u2014such as transactions, sales, customer information, sales activities, campaign details, and web analytics\u2014while enabling advanced analytics, operational efficiency, and risk management.\n\n**Tools & Technologies:**  \nThe solution leveraged a comprehensive AWS-based technology stack, including:\n- **AWS S3** for the Enterprise Data Lake (EDL)\n- **AWS Redshift** as the MPP (Massively Parallel Processing) data warehouse\n- **Apache Spark** for data integration processes\n- **AWS EMR, Lambda, and Step Functions** for serverless and scalable data processing\n- **AWS Data Pipeline** for orchestration\n- **Tableau** for analytical reporting and dashboarding\n- **GIT/Jenkins** for continuous integration and deployment (CI/CD)\n\n**Solution & Implementation:**  \nThe program established a future-ready enterprise data architecture and set up data management operations to support evolving business use cases. The data hub was designed to:\n- Seamlessly integrate data from legacy transaction systems, Salesforce, campaign activities, and web analytics\n- Support use cases such as sales management dashboards, territory optimization, campaign and sales effectiveness analysis, and product performance tracking\n- Reduce reliance on legacy systems by improving the data model to enable advanced analytics\n\n**Outcomes & Impact:**  \nThe implementation delivered significant business and technical benefits:\n- **Reduced data processing timelines** through optimized, serverless data workflows\n- **Lower infrastructure costs** by leveraging AWS\u2019s serverless architecture\n- **Highly optimized and scalable data warehouse** (AWS Redshift) for analytical workloads\n- **Cost-effective and scalable data engineering framework** using modern cloud-native tools\n- **Improved data integration** across legacy and modern systems, enabling comprehensive analytics and reporting\n\n**Business Value:**  \nThe enterprise data hub increased business value by driving revenue growth, enhancing operational efficiency, and strengthening risk management. The modernized data architecture positioned the firm to better support advanced analytics and future business requirements.\n\n**Relevance:**  \nBy integrating disparate data sources and reducing dependency on legacy systems, the firm now benefits from a unified, scalable, and analytics-ready data environment, supporting both current and future strategic initiatives.",
    "category": "Data Integration"
  },
  "Nike Data Foundation Case Study 2.pptx": {
    "file_name": "Nike Data Foundation Case Study 2.pptx",
    "project_title": "Nike Data Foundation",
    "domain": "Finance",
    "client_name": "Nike",
    "technology_used": "AWS, Snowflake",
    "summary": "**Nike Data Foundation Case Study Summary (Feb 2022)**\n\n**Overview and Objectives**  \nNike, the world\u2019s largest athletic footwear and apparel company (Fortune 100, ~$45B+ revenue), embarked on a finance transformation initiative to enable growth in its \u201cdirect-to-consumer\u201d (DTC) channel. This transformation was driven by the need for cloud-based data and analytics to support Nike\u2019s Direct strategic agenda, especially in the wake of significant business disruptions caused by the COVID-19 pandemic.\n\n**Challenges**  \n- **Pandemic Impact:** In Q4 2020, Nike experienced a sharp 38% decline in sales due to retail store closures, supply chain disruptions, and canceled sporting events.\n- **Financial Pressure:** The SG&A (Selling, General & Administrative) to Revenue year-over-year ratio was declining, indicating inefficiencies.\n- **Fragmented Systems:** Nike\u2019s pivot to a DTC model was hampered by fragmented global systems, inconsistent practices, and siloed data.\n- **Legacy Limitations:** Existing applications were not designed to handle the scale and complexity of enterprise-wide analytics required for the evolving business.\n\n**Solution: Data & Analytics Transformation**  \nNike established a Data and Analytics Center of Excellence (CoE), leveraging AWS and Snowflake cloud platforms to modernize its data infrastructure. The transformation was domain-led, focusing on finance as a business partner and enabling agility, predictive and prescriptive analytics, and new operating models. Key elements included:\n- **Depth of Domain Expertise:** 126 domain-specific use cases were identified to ensure business impact.\n- **On-Demand Visibility:** Real-time access to $45B+ of business data.\n- **Data to Insight, Insight to Action:** Streamlined processes from data collection to actionable insights.\n\n**Key Impact Areas and Results (Year 1):**\n\n1. **Accounts Receivable (AR) Analytics**\n   - **Cash Flow Benefits:** ~$450M annual improvement.\n   - **Receivables Management:** Real-time visibility into ~$3.5B in trade receivables, including order placement, delivery, returns, billing, credit card processing, and dispute management.\n   - **Operational Efficiency:** Enabled a 200+ person organization to manage AR with dashboards, canned reports, and self-service analytics.\n   - **DSO Reduction:** 5-day reduction in Days Sales Outstanding.\n   - **Customer Payment Deductions:** Over $50M reduction.\n\n2. **SG&A (Selling, General & Administrative) Analytics**\n   - **Actionable Insights:** $4B in SG&A spend analyzed for efficiency.\n   - **Expense Management:** Detailed planning and analytics for wage-related expenses, contract labor, and other overheads.\n   - **Predictive Analytics:** Scenario modeling to respond to changing business environments and ensure industry-leading SG&A-to-revenue efficiency.\n\n3. **Revenue, Cost, and Margin Analytics**\n   - **Revenue Visibility:** On-demand, granular visibility into $45B+ in annual revenue.\n   - **P&L Performance:** Enhanced forecasting and analysis by fiscal period, channel, customer segment, product, gross margin, cost of goods, and more.\n   - **Agility:** Improved ability to respond to business changes and reduced manual effort in data collection and analysis.\n\n**Technological Foundation**\n- **Cloud Platforms:** AWS and Snowflake provided scalable, secure, and high-performance data and analytics infrastructure.\n- **Self-Service Analytics:** Empowered business users with tools for slicing, dicing, and real-time reporting.\n\n**Conclusion**  \nNike\u2019s data foundation transformation enabled significant financial and operational benefits in its first year, including hundreds of millions in cash flow improvements, reduced payment deductions, and enhanced SG&A efficiency. The initiative provided Nike with the agility, insight, and predictive capabilities needed to thrive in a rapidly changing retail environment and to accelerate its direct-to-consumer growth strategy.",
    "category": "Data Analytics"
  },
  "Reference Data Master and Customer Registry helps Major Cable Provider Case Study 1.pptx": {
    "file_name": "Reference Data Master and Customer Registry helps Major Cable Provider Case Study 1.pptx",
    "project_title": "Reference Data Master and Customer Registry helps Major Cable Provider be CCPA Compliant",
    "domain": "Compliance",
    "client_name": "Unknown",
    "technology_used": "Not Mentioned",
    "summary": "**Summary of the Case Study: CCPA Compliance for a Major Cable Provider**\n\n**Challenges:**\nThe California Consumer Privacy Act (CCPA) mandates that California residents have the right to know what personal information (PI) businesses collect about them, access detailed reports, and request deletion of their data. For a major cable provider, this posed several challenges:\n- The need to scan all data warehouse applications to identify and extract customer-related personal data in response to consumer requests.\n- The requirement to provide consumers with clean, validated, and accurate reports, free from duplicates and inconsistencies, and standardized according to business rules.\n- The absence of a unified solution to deliver a consolidated and cleansed summary of PI attributes stored across various business applications.\n\n**Solution:**\nTo address these challenges, the cable provider implemented a Master Data Management (MDM) solution, comprising:\n- **MDM Reference Master:** Integrated all enterprise source applications to collect CCPA-impacted reference attributes and derive business translations for inclusion in consumer reports.\n- **MDM Customer Registry:** Audited and logged all consumer data rights activities (Right to Know, Right to Delete, Right to Opt Out), and stored unique consumer identifiers such as name and address.\n- **MDM Rules Master:** Established rules for customer data identification, deduplication, and survivorship to ensure the \"best version of truth\" for PI attributes in consumer reports.\n\n**Impact Delivered:**\n- The MDM Reference Master became an enterprise-wide, reusable framework for business value translations.\n- The solution enabled the cable provider to achieve CCPA compliance by delivering correct, accurate, and validated consumer reports.\n- The MDM platform facilitated data remediation and governance, allowing for the rapid onboarding of any CCPA-compliant PI attributes.\n\n**Conclusion:**\nBy leveraging MDM Reference Data Master and Customer Registry, the major cable provider successfully addressed CCPA compliance requirements, ensuring consumers receive comprehensive, accurate, and standardized reports of their personal information, while establishing a scalable framework for ongoing data governance and privacy management.",
    "category": "Master Data Management"
  },
  "Wabtec Data Lake Separation Case Study 2.pptx": {
    "file_name": "Wabtec Data Lake Separation Case Study 2.pptx",
    "project_title": "Large Scale Data Lake Set up & Migration Case Study",
    "domain": "Transportation",
    "client_name": "Unknown",
    "technology_used": "AWS, Chef, Hadoop",
    "summary": "**Comprehensive Summary: Large Scale Data Lake Set Up & Migration Case Study**\n\n**Background & Challenges:**\nA global organization operating across diverse sectors\u2014including freight, transit, electronics, equipment, services, and digital solutions for industries such as locomotive, mining, marine, stationary power, and drilling\u2014faced the complex task of merging business operations. The merger aimed to create a leading provider of equipment, aftermarket services, and digital solutions in the transportation sector, with a global presence in 50 countries. Key business objectives included improved revenue, enhanced client service, better utilization, accelerated automation, improved customer outcomes, and increased shareholder value.\n\nThe technical and operational challenges were significant:\n- The need to migrate a massive, business-critical data lake (over 112 TB) spread across 30+ technologies worldwide.\n- A strict timeline to complete the Transition Service Agreement (TSA) before September 2020, with a penalty of $300,000 per month for delays.\n- The requirement for a \"lift and shift\" migration of the entire data lake and consumption platform, ensuring zero data or object fidelity loss.\n- The necessity to harmonize data sets (e.g., suppliers, customers) to drive synergy benefits and support ambitious merger targets.\n\n**Solution Approach:**\nTo address these challenges, a comprehensive strategy was developed, encompassing both top-down and bottom-up discovery phases. Key elements of the solution included:\n\n- **Strategic Planning:** Detailed discovery to map out business and technical requirements, with a focus on common KPIs to realize synergy benefits through harmonized data sets.\n- **Cloud Migration:** Leveraged Genpact\u2019s cloud assets and expertise for a high-quality, low-risk, and accelerated migration process.\n- **Architecture:** Designed a scalable, flexible architecture using AWS as Infrastructure-as-a-Service (IaaS) and open-source technologies to minimize costs.\n- **Migration Strategy:** Developed multiple migration options to address customer security constraints, minimize application downtime, and ensure zero data loss.\n- **Automation:** Employed Chef scripts for IaaS automation, enabling rapid provisioning of servers and platforms.\n\n**Impact & Results:**\nThe project delivered substantial business and technical benefits:\n- **Seamless Transition:** Achieved a smooth migration to the new, combined platform, realizing the intended synergy value.\n- **On-Time Delivery:** Met the TSA deadline with zero business disruption, avoiding costly penalties.\n- **Automation & Efficiency:** Automated AWS cloud platform setup using Infrastructure as Code (IaC), with 80%+ automation for installation and configuration of consumption tools, and 10-15% for Hadoop.\n- **Agility:** Enabled the ability to spin up new clusters within a single day.\n- **Performance:** Achieved near real-time data ingestion of 100,000 records with minimal latency.\n- **Quality Assurance:** Implemented automated comparison of over 4,000 Hadoop platform parameters and table migrations to ensure data integrity.\n\n**Additional Case Study: Post-trade Financial Services Company**\nA leading post-trade financial services provider, specializing in clearing and settlement, was also supported. The solution involved:\n- Designing a layered solution architecture (ingestion, ODS, staging, integration, data mart, presentation).\n- Recommending a systematic approach to reduce total cost of ownership and enable clear cost accounting.\n\n**Conclusion:**\nThe case study demonstrates a successful large-scale data lake migration and platform setup, overcoming complex technical, operational, and business challenges. Through strategic planning, cloud-based architecture, automation, and rigorous quality assurance, the organization achieved a seamless transition, improved operational efficiency, and realized significant business value from the merger.",
    "category": "Data Migration"
  }
}
